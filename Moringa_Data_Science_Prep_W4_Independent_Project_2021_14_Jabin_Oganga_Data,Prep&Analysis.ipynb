{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moringa_Data_Science_Prep_W4_Independent_Project_2021_14_Jabin_Oganga_Data,Prep&Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOW6/TIyVrZkmRRPzzPOOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JABINOGANGA/Moringa_Data_Science_Prep_W4_Independent_Project_2021_14_Jabin_Oganga_Data-Prep-Analysis.ipynb/blob/main/Moringa_Data_Science_Prep_W4_Independent_Project_2021_14_Jabin_Oganga_Data%2CPrep%26Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTbZvbBktv1L"
      },
      "source": [
        "## Importing Libraries\n",
        "# Pandas & Numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoSw0HEMuGjm"
      },
      "source": [
        "# Load the two data sets into the notebook\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPZ3EFUuaCO"
      },
      "source": [
        "## Loading DataSets & Preveiwing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmLfRtmXul2y"
      },
      "source": [
        "# Load the two data sets into the notebook then illustrate a preview by just listing 10 rows using the head function\n",
        "data_df = pd.read_csv('/content/Autolib_dataset (2) (1).csv' ) \n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzgFjczbunmQ"
      },
      "source": [
        "# Load the two data sets into the notebook then illustrate a preview by just listing 10 rows using the head functio\n",
        "det_df = pd.read_excel('/content/Stations_Open_Data.xlsx' ) \n",
        "det_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-41fxaWvYht"
      },
      "source": [
        "# Accessing Information about our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvcaR5SlvjEs"
      },
      "source": [
        "def describe(dataframe):\n",
        "  print(dataframe.shape , dataframe.info() )\n",
        "\n",
        "describe(data_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ntLnRXwHgk"
      },
      "source": [
        "describe(det_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvT6XZ2Owvg9"
      },
      "source": [
        "# Cleaning our Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqtRS4R8vBN9"
      },
      "source": [
        "Validity & Completeness\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lByTE2P-RHV"
      },
      "source": [
        "# In this section we scheme through the table to check for irrelevant columns \n",
        "# This includes columns with a lot of unfilled null(unfilled) rows and columns with data which is unnecessary for our problem statement\n",
        "# The scheduled at column has no data just as it was not filled therefore we get rid of that column by dropping it as it is not relevant to our case study \n",
        " \n",
        "# print(data_df.isnull().any()) #To check the columns with null values\n",
        "\n",
        "# print(data_df.isnull()) #To check in the rows where there are null values\n",
        "\n",
        "# print(data_df.isnull().sum()) #To check in the total number of null values in the entire data set\n",
        "\n",
        "\n",
        "# We therefore define a function to describe the null values in our data\n",
        "def null_values(dataframe):\n",
        "  print(dataframe.isnull().any(), dataframe.isnull().head(5) , print(dataframe.isnull().sum()) )\n",
        "\n",
        "null_values(data_df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U82eqW-b-m9u"
      },
      "source": [
        "# Now that we've gathered the columns with null values and schemed through the list for irrelevant columns\n",
        "# like the month and year column as they have the simillar information all through we proceed to\n",
        "# Dropping the unessesary columns and the ones that lack values that could be used for our case study this includes the displayed comments and schedule at\n",
        "# columns as they have missing a lot of missing values and this will interfere with our model's accuracy\n",
        "\n",
        "data = data_df.drop(['Displayed comment','Scheduled at'],axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9AauOGQN-n"
      },
      "source": [
        "# We preview the dataset to make sure the \n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwQO1mjsTVLL"
      },
      "source": [
        "# We need to create a new column that combines the day time and month to to make the data more ellaborate and reduce repetition\n",
        "data['Date-Time'] = data['hour'].astype(str) + ':' + data['minute'].astype(str) + 'hrs' + ' ' + 'on'+ ' ' + data['day'].astype(str)+ ' ' + 'April 2018'\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2bEmdLplQBW"
      },
      "source": [
        "# We then drop the columns that have been combined to create one elaborate one\n",
        "data = data.drop(['year','hour','minute','day', 'month'], axis = 1)\n",
        "data.head(5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3OyYQXiRgUw"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cfMy3O7ZadR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32fsHNQ3RZOr"
      },
      "source": [
        "# We need to ensure that the values are arthmetically correct \n",
        "\n",
        "data.describe(include= 'all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0upGqwlErwOA"
      },
      "source": [
        "Consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHF048UrZUy"
      },
      "source": [
        "# In order to avoid repetition of data we scheme through the entire table for repeated rows\n",
        "# We use the drop function to excecute this\n",
        "data = data.drop_duplicates()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piyw2dAmsp7t"
      },
      "source": [
        "Uniformity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLH_5kiJtLU2"
      },
      "source": [
        "# To ensure the data is well alligned in all the columns we use the trailing functions on the adress column to align the data on one line as it is address information which might be confused if read incorrectly\n",
        "data['Address'] = data['Address'].str.replace(' ','')\n",
        "# data['Address'] = data['Address'].str.split(' , ')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh0I-ofnspOR"
      },
      "source": [
        "# For more uniformity we capitalize each column on axis =1 to ensure the data looks presentable\n",
        "def upper(dataframe):\n",
        "  dataframe.columns= dataframe.columns.str.upper()\n",
        "  return print(dataframe.columns)\n",
        "\n",
        "upper(data) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kda4wNRTywct"
      },
      "source": [
        "Validity & Completeness II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjVpdYptywD8"
      },
      "source": [
        "det_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92P2uHD9yuGi"
      },
      "source": [
        "# In this section we scheme through the table to check for irrelevant columns \n",
        "# This includes columns with a lot of unfilled null(unfilled) rows and columns with data which is unnecessary for our problem statement\n",
        "# The scheduled at column has no data just as it was not filled therefore we get rid of that column by dropping it as it is not relevant to our case study \n",
        "# def null_values(datas):\n",
        "#   print(datas.isnull().any(), datas.isnull().head(5) , print(datas.isnull().sum()) )\n",
        "\n",
        "null_values(det_df) # We used the defined value to execute this command\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e86mAJ0I6x2D"
      },
      "source": [
        "# we scheme through the table to check for irrelevant columns and  \n",
        "# This includes columns with a lot of missing/null(unfilled) rows and columns with data which is unnecessary for our problem statement\n",
        "# The scheduled at column has no data just as it was not filled therefore we get rid of that column by dropping it as it is not relevant to our case study\n",
        "# The column on abri which implies to 'abri'(shelter) and the 'prises tiers'(Tire prices) have most null values and are conviniently not relevant to our problem statement therefore we need to drop them\n",
        "det_df = det_df.drop(['prises Tiers T3','abri'], axis = 1)\n",
        "det_df.head(5) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbUMd4yt9nrH"
      },
      "source": [
        "Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxErQTJ39qo0"
      },
      "source": [
        "# We need to ensure that the values are arthmetically correct \n",
        "\n",
        "det_df.describe(include= 'all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4axIhw2M-OyD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ue8dpds-tyt"
      },
      "source": [
        "Consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DgHLwdQ-z7n"
      },
      "source": [
        "# We use the drop function to track and drp repeated rows through out the dataset \n",
        "det = det_df.drop_duplicates()\n",
        "det.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RqjR8w29XQE"
      },
      "source": [
        "Uniformity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x26Q9B0o_vqy"
      },
      "source": [
        "# We apply the  defined upper function to capialize each column name on axis = 1 to make the dataframe look presentable and compatible for merging\n",
        "\n",
        "\n",
        "upper(det) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVAn3oXTDpa9"
      },
      "source": [
        "## Merging the two DataSets for Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDQ_BRmF8iZb"
      },
      "source": [
        "# We rename the specific columns on the to help merge the data frames using colums with simmilar row values and column names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh4kDuj6OVrm"
      },
      "source": [
        "data.rename(columns={'ADDRESS':'ADRESSE' },inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxSI1XDyVVVm"
      },
      "source": [
        "det.rename(columns={'CP':'POSTAL CODE'},inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uqtctn0D23m"
      },
      "source": [
        "combo = pd.merge(det,data ,left_on= 'POSTAL CODE', right_on= 'POSTAL CODE' )\n",
        "combo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga5YRDndWwXc"
      },
      "source": [
        "# Overview\n",
        "\n",
        "In this week's independent project, you will be working as a data scientist working for an electric car-sharing service company. You have been tasked to process stations data to understand electric car usage over time by solving for the following research question;\n",
        "\n",
        "## Research Question\n",
        "\n",
        "Identify the most popular hour of the day for picking up a shared electric car (Bluecar) in the city of Paris over the month of April 2018.\n",
        "Bonus Questions (Optional)\n",
        "\n",
        "What is the most popular hour for returning cars?\n",
        "What station is the most popular?\n",
        "Overall?\n",
        "- At the most popular picking hour?\n",
        "- What postal code is the most popular for picking up Blue cars? - - Does the most popular station belong to that postal code?\n",
        "  - Overall?\n",
        "- At the most popular picking hour?\n",
        "- Do the results change if you consider Utilib and Utilib 1\n",
        "  instead of Blue cars? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT_7ob3msPea"
      },
      "source": [
        "# Identify the most popular hour of the day for picking up a shared electric car (Bluecar) in the city of Paris over the month of April 2018\n",
        "\n",
        "\n",
        "pick = combo['VILLE'] == 'Paris' # Groupby the city\n",
        "x = combo.loc[pick]['BLUECAR COUNTER'].max() # Select the dataset you want to focus on\n",
        "result = combo.loc[x, 'DATE-TIME'] # GGet the specific time on that selected dataset\n",
        "result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4GE6ZUu_7mK"
      },
      "source": [
        "# What is the most popular hour for returning cars\n",
        "combo['DATE-TIME'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCFqd_8FaNY"
      },
      "source": [
        "# Most popular picking hours\n",
        "most = (combo['BLUECAR COUNTER'] + combo['UTILIB COUNTER'] + combo['UTILIB 1.4 COUNTER']).max()\n",
        "hour = combo.loc[most, 'DATE-TIME']\n",
        "hour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rZSlPfgFjnX"
      },
      "source": [
        "# What postal code is the most popular for picking up Blue cars\n",
        "blue = combo['BLUECAR COUNTER']>=1\n",
        "postel = combo.loc[blue, 'POSTAL CODE']\n",
        "postel.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFKJiezZFyie"
      },
      "source": [
        "# Most popular station overally\n",
        "most = (combo['BLUECAR COUNTER'] + combo['UTILIB COUNTER'] + combo['UTILIB 1.4 COUNTER']).max() # find the column that had the most car hire \n",
        "station = combo.loc[most, 'PUBLIC NAME'] # locate the station where this care hire took place\n",
        "station"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddpkVOiAF4zq"
      },
      "source": [
        "# The postal code to most popular station postal code\n",
        "combo.loc[most , 'POSTAL CODE']\n",
        "\n",
        "# In conclusion it is not the same as the one used to order blue cars "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVAmgV7AGZ1-"
      },
      "source": [
        "# Most popular picking hours\n",
        "most = (combo['BLUECAR COUNTER'] + combo['UTILIB COUNTER'] + combo['UTILIB 1.4 COUNTER']).max()\n",
        "hours = combo.loc[most, 'DATE-TIME']\n",
        "hours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQuYGKT2GcrM"
      },
      "source": [
        "# The data changes when you consider Utilib 1.4 and utilib \n",
        "trans = (combo['UTILIB COUNTER'] + combo['UTILIB 1.4 COUNTER']).max()\n",
        "ehours = combo.loc[trans, 'DATE-TIME']\n",
        "ehours\n",
        "\n",
        "# In conclusion it changes when you remove blue cars"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}